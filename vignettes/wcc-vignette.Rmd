---
title: "Introduction to weighted consensus clustering"
author: "Can Huang"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    theme: united
vignette: >
  %\VignetteIndexEntry{R package wcc}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


The R package **wcc** (**w**eighted **c**onsensus **c**lustering) introduces weighted consensus clustering functions inspired by consensus clustering methods.

The main function in this package is `WeightedConsensusCluster`. Users input a list of datasets, clusteing methods, the number of clutsers for separate datasets and global, criterions and so on. Then consensus clustering will be applied to each dataset to derive consensus matrix. Depends on one layer or two-layer, consensus matrix will be weighted and combined for each dataset and final clustering. Finally user could choose pam or hierarchical clustering method to derive the clustering results. Some useful functions are also included in this package: 

* `criterion`. This function takes a list of consensus matrix, a list of corresponding clustering label results and a vector of corresponding number of clusters. It could use Silhouette, widestGap, dunns and dunn2s index as criterion to choose the best number of clusters.

* `weightcal`. This function takes a list of consensus matrix and clustering label results. Then weights are calculated based on ratio of in-cluster proportion to out-of-cluster proportion using the cluster estimated by the algorithm itself. It could help to estimate weights for different methods of each methods or weights for consensus matrix of each dataset.

The other function needed for this is `consensusCluster`. This function can be found in the R package `coca` and is used to perform consensus clustering on one dataset and obtain a co-clustering matrix  (Monti et al. 2003). We made some changes to this function to allow users to define more distance metric and sample percentage for features.


# How wcc works 

First, we generate four different type datasets with the same clustering structure (3 clusters of equal size) and different levels of noise.

```{r generate data, fig.show='hold', eval=FALSE,warning=FALSE, cache=TRUE}
n1=20
n2=20
n3=20
n = n1+n2+n3
p=30
p1 =5
p2 = 5
p3 = 5

true.class=c(rep(1,n1),rep(2,n2),rep(3,n3))
  ################# normal distribution ####################
  c1=matrix(rnorm(n1*p1,mean=2.2,sd=1.7), ncol=p1,nrow=n1)
  c2=matrix(rnorm(n2*p2,mean=1.6, sd=1.7), ncol=p2,nrow=n2)
  c3=matrix(rnorm(n3*p3,mean=1, sd=1.7), ncol=p3,nrow=n3)
  
  normData = matrix(rnorm(n*p, mean=0, sd=1.7),nrow=n, ncol=p)
  normData[1:n1,1:p1]= c1
  normData[(n1+1):(n1+n2),(p1+1):(p1+p2)]= c2
  normData[(n1+n2+1):n,(p1+p2+1):(p1+p2+p3)]= c3
  
  noiseData = matrix(rnorm(n*p, mean=0, sd=1),nrow=n, ncol=p)
  ######## binomial data #########################
  c1=matrix(rbinom(n1*p1,size=1, prob=0.54), ncol=p1,nrow=n1)
  c2=matrix(rbinom(n2*p2,size=1, prob=0.42), ncol=p2,nrow=n2)
  c3=matrix(rbinom(n3*p3,size=1, prob=0.3), ncol=p3,nrow=n3)
  
  binomData = matrix(rbinom(n*p, size=1, prob=0.1),nrow=n, ncol=p)
  binomData[1:n1,1:p1]= c1
  binomData[(n1+1):(n1+n2),(p1+1):(p1+p2)]= c2
  binomData[(n1+n2+1):n,(p1+p2+1):(p1+p2+p3)]= c3
  
  ######### poisson data ############################
  c1=matrix(rpois(n1*p1,lambda=1.9), ncol=p1,nrow=n1)
  c2=matrix(rpois(n2*p2,lambda=1.5), ncol=p2,nrow=n2)
  c3=matrix(rpois(n3*p3,lambda=1), ncol=p3,nrow=n3)
  
  poisData = matrix(rpois(n*p, lambda=0.6),nrow=n, ncol=p)
  poisData[1:n1,1:p1]= c1
  poisData[(n1+1):(n1+n2),(p1+1):(p1+p2)]= c2
  poisData[(n1+n2+1):n,(p1+p2+1):(p1+p2+p3)]= c3

  ################## multinomial distribution ##############
  c1=matrix(sample(1:3,size=n1*p1, replace=TRUE, prob=c(0.8,0.15,0.05)), ncol=p1,nrow=n1)
  c2=matrix(sample(1:3,size=n2*p2, replace=TRUE, prob=c(0.3,0.4,0.3)), ncol=p2,nrow=n2)
  c3=matrix(sample(1:3,size=n3*p3, replace=TRUE, prob=c(0.05,0.15,0.8)), ncol=p3,nrow=n3)
  
  multData = matrix(sample(1:3,size=n*p, replace=TRUE, prob=c(0.33,0.33,0.33)),nrow=n, ncol=p)
  multData[1:n1,1:p1]= c1
  multData[(n1+1):(n1+n2),(p1+1):(p1+p2)]= c2
  multData[(n1+n2+1):n,(p1+p2+1):(p1+p2+p3)]= c3
  
  
  #biID = seq(1,p,2)
  #multData[,biID] = binomData[,biID]
  
  ### some variables only have one category; need to remove these variables
  ncat = function(x){length(unique(x))}
  len = apply(binomData,2,ncat)
  unicat = which(len==1)
  if (length(unicat)>0) {
    binomData = binomData[,-unicat]
  }
  


```

Now we can use the `consensusclustering` function to compute a consensus matrix for each dataset.

```{r consensus_cluster, fig.show='hold', eval=FALSE,warning=FALSE, cache=TRUE}
  normRes=consensuscluster(normData,K=3,B=1000,pItem = 0.8,pFeature = 0.8 ,clMethod ="kmeans",finalclmethod="pam")
  binomRes=consensuscluster(binomData,K=3,B=1000,pItem = 0.8,pFeature = 0.8 ,clMethod ="kmeans",finalclmethod="pam")
  poisRes=consensuscluster(lopoisData,K=3,B=1000,pItem = 0.8,pFeature = 0.8 ,clMethod ="kmeans",finalclmethod="pam")
  multRes=consensuscluster(multData,K=3,B=1000,pItem = 0.8,pFeature = 0.8 ,clMethod ="kmeans",finalclmethod="pam")

  normRes1=consensuscluster(normData,K=3,B=1000,pItem = 0.8,pFeature = 0.8 ,clMethod ="hclust",finalclmethod="pam")
  binomRes1=consensuscluster(binomData,K=3,B=1000,pItem = 0.8,pFeature = 0.8 ,clMethod ="hclust",dist = "binary",finalclmethod="pam")
  poisRes1=consensuscluster(poisData,K=3,B=1000,pItem = 0.8,pFeature = 0.8 ,clMethod ="hclust",dist = "jaccard",finalclmethod="pam")
  multRes1=consensuscluster(multData,K=3,B=1000,pItem = 0.8,pFeature = 0.8 ,clMethod ="hclust",dist = "hamming",finalclmethod="pam")
```

Then we could perform one layer weighted consensus clustering.

```{r one layer wcc, fig.show='hold', eval=FALSE,warning=FALSE, cache=TRUE}
  res.all <- c(list(normRes),list(binomRes),list(poisRes),list(multRes))
  weights <-weightcal(res.all)
  wcm=weights[1]*res.all[[1]]$consensusMatrix+weights[2]*res.all[[2]]$consensusMatrix+
    weights[3]*res.all[[3]]$consensusMatrix+weights[4]*res.all[[4]]$consensusMatrix
  distances <- stats::as.dist(1 - wcm)
  weight_clusterLabels <- pam(distances,3,diss = TRUE,metric = "euclidean" )$clustering
  ari.weight<- adjustedRandIndex(true.class, weight_clusterLabels)
```

Or we could perform two-layer weighted consensus clustering.

```{r two-layer wcc, fig.show='hold', eval=FALSE,warning=FALSE, cache=TRUE}
  #norm
  res.norm <- c(list(normRes),list(normRes1))
  weight.norm<- weightcal(res.norm)
  wcm=weight.norm[1]*res.norm[[1]]$consensusMatrix+weight.norm[2]*res.norm[[2]]$consensusMatrix
  distances <- stats::as.dist(1 - wcm)
  weight_clusterLabels <-pam(distances,3,diss = TRUE,metric = "euclidean" )$clustering
  weightnormRes=list(consensusMatrix=wcm,class=weight_clusterLabels)
  
  #binom
  res.binom <- c(list(binomRes),list(binomRes1))
  weight.binom <- weightcal(res.binom)
  wcm=weight.binom[1]*res.binom[[1]]$consensusMatrix+weight.binom[2]*res.binom[[2]]$consensusMatrix
  distances <- stats::as.dist(1 - wcm)
  weight_clusterLabels <- pam(distances,3,diss = TRUE,metric = "euclidean" )$clustering
  weightbinomRes=list(consensusMatrix=wcm,class=weight_clusterLabels)
  
  #poisson
  res.pois <- c(list(poisRes),list(poisRes1))
  weight.pois <- weightcal(res.pois)
  wcm=weight.pois[1]*res.pois[[1]]$consensusMatrix+weight.pois[2]*res.pois[[2]]$consensusMatrix
  distances <- stats::as.dist(1 - wcm)
  weight_clusterLabels <-pam(distances,3,diss = TRUE,metric = "euclidean" )$clustering
  weightpoisRes=list(consensusMatrix=wcm,class=weight_clusterLabels)
  
  #mult
  res.mult <- c(list(multRes),list(multRes1))
  weight.mult <- weightcal(res.mult)
  wcm=weight.mult[1]*res.mult[[1]]$consensusMatrix+weight.mult[2]*res.mult[[2]]$consensusMatrix
  distances <- stats::as.dist(1 - wcm)
  weight_clusterLabels <- pam(distances,3,diss = TRUE,metric = "euclidean" )$clustering
  weightmultRes=list(consensusMatrix=wcm,class=weight_clusterLabels)
  
  #2 layer wcm
  res.wcm <- c(list(weightnormRes),list(weightbinomRes),list(weightpoisRes),list(weightmultRes))
  weight.wcm <- weightcal(res.wcm)
  wcm=weight.wcm[1]*res.wcm[[1]]$consensusMatrix+weight.wcm[2]*res.wcm[[2]]$consensusMatrix+
    weight.wcm[3]*res.wcm[[3]]$consensusMatrix+weight.wcm[4]*res.wcm[[4]]$consensusMatrix
  distances <- stats::as.dist(1 - wcm)
  weight_clusterLabels <- pam(distances,3,diss = TRUE,metric = "euclidean" )$clustering
  end.time3=Sys.time()
  ari.wcm <- adjustedRandIndex(true.class, weight_clusterLabels)
```


The same can be done simply using the function `WeightedConsensusCluster`

```{r klic, fig.show='hold', warning=FALSE, eval=FALSE,cache=TRUE}
#one layer
  data_for_wcc <- list()
  data_for_wcc[[1]]=normData
  data_for_wcc[[2]]=binomData
  data_for_wcc[[3]]=poisData
  data_for_wcc[[4]]=multData
  wcc <- WeightedConsensusCluster(data_for_wcc, method="one layer", individualK = rep(3, 4),
                                   globalK = 3, pFeature = 0.8 ,ccClMethods = "k-means",
                                    ccDistHCs = "euclidean",hclustMethod = "average",finalclmethod="hclust",
                                    finalhclustMethod = "average",Silhouette=TRUE)

#two-layer
  data_for_wcc <- list()
  data_for_wcc[[1]]=normData
  data_for_wcc[[2]]=binomData
  data_for_wcc[[3]]=lopoisData
  data_for_wcc[[4]]=multData
  data_for_wcc[[5]]=normData
  data_for_wcc[[6]]=binomData
  data_for_wcc[[7]]=poisData
  data_for_wcc[[8]]=multData
  wcc <- WeightedConsensusCluster(data_for_wcc, method="one layer", individualK = rep(3, 8),
                     globalK = 3, pFeature = 0.8 ,ccClMethods = c("kmeans","kmeans","kmeans","kmeans",
                                                                  "hclust","hclust","hclust","hclust"),
                     ccDistHCs = c("euclidean","euclidean","euclidean","euclidean",
                                   "euclidean","binary","jaccard","hamming"),hclustMethod = "average",finalclmethod="hclust",
                                    finalhclustMethod = "average",Silhouette=TRUE)

```



# References 

Cabassi, A. and Kirk, P. D. W. (2019). Multiple kernel learning for integrative
consensus clustering of 'omic datasets. arXiv preprint. arXiv:1904.07701.


